# Número de particiones y repeticiones
particiones  <- 10
repeticiones <- 10
hiperparametros <- expand.grid(.model = "tree",
.winnow = FALSE,
.trials = 15:40,
.rules = c(0.5, 1, 2)
)
set.seed(123)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)
for (i in 1:(particiones * repeticiones)) {
seeds[[i]] <- sample.int(1000, nrow(hiperparametros))
}
seeds[[(particiones * repeticiones) + 1]] <- sample.int(1000, 1)
control_train <- trainControl(method = "repeatedcv", number = particiones,
repeats = repeticiones, seeds = seeds,
returnResamp = "final", verboseIter = FALSE,
allowParallel = TRUE)
set.seed(342)
modelo_C5.0 <- caret::train(BH.staging ~ ., data = data_tn2,
method = "C5.0",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control_train)
knitr::opts_chunk$set(echo = TRUE, comment = NULL,
warning = FALSE, message = FALSE,
fig.align="center", cache = TRUE)
# Cargamos los paquetes
library(knitr)
library(caret)
library(class)
library(e1071)
library(neuralnet)
library(NeuralNetTools)
library(kernlab)
library(randomForest)
library(psych)
library(ggplot2)
library(gridExtra)
library(ggpubr)
library(dplyr)
library(bookdown)
library(prettydoc)
library(corrplot)
library(ggcorrplot)
library(GGally)
library(randomForestSRC)
library(tibble)
library(recipes)
library(tidyr)
library(outliers)
library(RSNNS)
library(ranger)
library(lattice)
library(C50)
# Cargamos los datos
setwd("D:/DRIVE UNIVERSIDAD/UOC/Sem4/PEC3/github/modelos")
set.seed(2312)
m.file <- file.path(params$folder.data, params$myfile)
load(m.file)
head(HCV4)
#Función
normalizar <- function(x) {
return((x -min(x)) / (max(x) -min(x)))
}
# Particion de los datos train/test
## Muestras que pertenecen al conjunto train
set.seed(77)
n_train <- createDataPartition(HCV4$BH.staging, p = 0.7, list = FALSE)
## Eliminamos las variables con menos importancia
set.seed(77)
HCV2<- HCV4
HCV2 <- HCV2 %>%
select(-Epigastria.pain, -Headache, -Nausea.Vomiting, -Jaundice, -Fatigue.Boneache, -Fever, -Diarrhea, -RNA.EOT, -RNA.EF, -HGB)
# -BMI, -BH.grading, -BMI, -ALT.after.24.w, -ALT.36, -ALT.12, -ALT.48, -ALT.24, -ALT.4, -RNA.4)
#, -RNA.12, -ALT.after.24.w, -ALT.36, -ALT.12, -ALT.48, -ALT.24, -ALT.4, -RNA.4, -RNA.EOT, -RNA.EF, -HGB)
## Creamos los conjuntos train y test:
train.d <- HCV2[n_train,]
test.d <- HCV2[-n_train,]
## Variable objetivo por conjunto de datos
label_train <- HCV2[n_train,length(HCV2)]
label_test <- HCV2[-n_train,length(HCV2)]
# Preparamos los conjuntos de datos para que sean preprocesados
# Creamos un respaldo de los datos de prueba y entrenamiento
data_train <- train.d
data_test  <- test.d
data_tn <- data_train
data_tt <- data_test
data_tn <- data_tn %>%
select(-Age, -Gender)
data_tt <- data_tt %>%
select(-Age, -Gender)
# Normalizamos los datos de prueba y entrenamiento
data_tn2 <- data_tn %>%
mutate_if(is.numeric, normalizar)
data_tt2 <- data_tt %>%
mutate_if(is.numeric, normalizar)
# Obtener los valores mínimos y máximos de las variables numéricas en data_train
#min_val <- data_tn %>%
# summarise(across(where(is.numeric), min))
#max_val <- data_tn %>%
# summarise(across(where(is.numeric), max))
#save(data_train, data_test, min_vals, max_vals, file = "datos.RData")
# se recurre a validación cruzada repetida como método de validación.
# Número de particiones y repeticiones
particiones <- 10
repeticiones <- 10
hiperparametros <- expand.grid(.model = "tree", .winnow = FALSE, .trials = 15:50)
set.seed(123)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)
for (i in 1:(particiones * repeticiones)) {
seeds[[i]] <- sample.int(1000, nrow(hiperparametros))
}
seeds[[(particiones * repeticiones) + 1]] <- sample.int(1000, 1)
control_train <- trainControl(
method = "repeatedcv",
number = particiones,
repeats = repeticiones,
seeds = seeds,
returnResamp = "final",
verboseIter = FALSE,
allowParallel = TRUE
)
set.seed(342)
modelo_C5.0 <- caret::train(
BH.staging ~ .,
data = data_train,
method = "C5.0",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control_train
)
modelo_C5.0
#summary(modelo_C5.0$finalModel)
ggplot(modelo_C5.0, highlight = TRUE) +
labs(title = "Evolución del accuracy del modelo") +
theme_bw()
# Predicciones
predicciones_C5.0 <- predict(modelo_C5.0, newdata = data_tt,
type = "raw")
knitr::opts_chunk$set(echo = TRUE, comment = NULL,
warning = FALSE, message = FALSE,
fig.align="center", cache = TRUE)
# Cargamos los paquetes
library(knitr)
library(caret)
library(class)
library(e1071)
library(neuralnet)
library(NeuralNetTools)
library(kernlab)
library(randomForest)
library(psych)
library(ggplot2)
library(gridExtra)
library(ggpubr)
library(dplyr)
library(bookdown)
library(prettydoc)
library(corrplot)
library(ggcorrplot)
library(GGally)
library(randomForestSRC)
library(tibble)
library(recipes)
library(tidyr)
library(outliers)
library(RSNNS)
library(ranger)
library(lattice)
library(C50)
# Cargamos los datos
setwd("D:/DRIVE UNIVERSIDAD/UOC/Sem4/PEC3/github/modelos")
set.seed(2312)
m.file <- file.path(params$folder.data, params$myfile)
load(m.file)
head(HCV4)
#Función
normalizar <- function(x) {
return((x -min(x)) / (max(x) -min(x)))
}
# Particion de los datos train/test
## Muestras que pertenecen al conjunto train
set.seed(77)
n_train <- createDataPartition(HCV4$BH.staging, p = 0.7, list = FALSE)
## Eliminamos las variables con menos importancia
set.seed(77)
HCV2<- HCV4
HCV2 <- HCV2 %>%
select(-Epigastria.pain, -Headache, -Nausea.Vomiting, -Jaundice, -Fatigue.Boneache, -Fever, -Diarrhea, -RNA.EOT, -RNA.EF, -HGB)
# -BMI, -BH.grading, -BMI, -ALT.after.24.w, -ALT.36, -ALT.12, -ALT.48, -ALT.24, -ALT.4, -RNA.4)
#, -RNA.12, -ALT.after.24.w, -ALT.36, -ALT.12, -ALT.48, -ALT.24, -ALT.4, -RNA.4, -RNA.EOT, -RNA.EF, -HGB)
## Creamos los conjuntos train y test:
train.d <- HCV2[n_train,]
test.d <- HCV2[-n_train,]
## Variable objetivo por conjunto de datos
label_train <- HCV2[n_train,length(HCV2)]
label_test <- HCV2[-n_train,length(HCV2)]
# Preparamos los conjuntos de datos para que sean preprocesados
# Creamos un respaldo de los datos de prueba y entrenamiento
data_train <- train.d
data_test  <- test.d
data_tn <- data_train
data_tt <- data_test
data_tn <- data_tn %>%
select(-Age, -Gender)
data_tt <- data_tt %>%
select(-Age, -Gender)
# Normalizamos los datos de prueba y entrenamiento
data_tn2 <- data_tn %>%
mutate_if(is.numeric, normalizar)
data_tt2 <- data_tt %>%
mutate_if(is.numeric, normalizar)
# Obtener los valores mínimos y máximos de las variables numéricas en data_train
#min_val <- data_tn %>%
# summarise(across(where(is.numeric), min))
#max_val <- data_tn %>%
# summarise(across(where(is.numeric), max))
#save(data_train, data_test, min_vals, max_vals, file = "datos.RData")
# se recurre a validación cruzada repetida como método de validación.
# Número de particiones y repeticiones
particiones <- 10
repeticiones <- 10
hiperparametros <- expand.grid(.model = "tree", .winnow = FALSE, .trials = 1:20)
set.seed(123)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)
for (i in 1:(particiones * repeticiones)) {
seeds[[i]] <- sample.int(1000, nrow(hiperparametros))
}
seeds[[(particiones * repeticiones) + 1]] <- sample.int(1000, 1)
control_train <- trainControl(
method = "repeatedcv",
number = particiones,
repeats = repeticiones,
seeds = seeds,
returnResamp = "final",
verboseIter = FALSE,
allowParallel = TRUE
)
set.seed(342)
modelo_C5.0 <- caret::train(
BH.staging ~ .,
data = data_train,
method = "C5.0",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control_train
)
modelo_C5.0
#summary(modelo_C5.0$finalModel)
ggplot(modelo_C5.0, highlight = TRUE) +
labs(title = "Evolución del accuracy del modelo") +
theme_bw()
# Predicciones
predicciones_C5.0 <- predict(modelo_C5.0, newdata = data_tt,
type = "raw")
setwd("D:/DRIVE UNIVERSIDAD/UOC/Sem4/PEC3/github/modelos")
knitr::opts_chunk$set(echo = TRUE, comment = NULL,
warning = FALSE, message = FALSE,
fig.align="center", cache = TRUE)
# Cargamos los paquetes
library(knitr)
library(caret)
library(class)
library(e1071)
library(neuralnet)
library(NeuralNetTools)
library(kernlab)
library(randomForest)
library(psych)
library(ggplot2)
library(gridExtra)
library(ggpubr)
library(dplyr)
library(bookdown)
library(prettydoc)
library(corrplot)
library(ggcorrplot)
library(GGally)
library(randomForestSRC)
library(tibble)
library(recipes)
library(tidyr)
library(outliers)
library(RSNNS)
library(ranger)
library(lattice)
library(keras)
library(mlr)
library(tensorflow)
setwd("D:/DRIVE UNIVERSIDAD/UOC/Sem4/PEC3/github/modelos")
# Cargamos los datos
set.seed(2312)
m.file <- file.path(params$folder.data, params$myfile)
load(m.file)
knitr::opts_chunk$set(echo = TRUE, comment = NULL,
warning = FALSE, message = FALSE,
fig.align="center", cache = TRUE)
# Cargamos los paquetes
library(knitr)
library(caret)
library(class)
library(e1071)
library(neuralnet)
library(NeuralNetTools)
library(kernlab)
library(randomForest)
library(psych)
library(ggplot2)
library(gridExtra)
library(ggpubr)
library(dplyr)
library(bookdown)
library(prettydoc)
library(corrplot)
library(ggcorrplot)
library(GGally)
library(randomForestSRC)
library(tibble)
library(recipes)
library(tidyr)
library(outliers)
library(RSNNS)
library(ranger)
library(lattice)
library(keras)
library(mlr)
library(tensorflow)
setwd("D:/DRIVE UNIVERSIDAD/UOC/Sem4/PEC3/github/modelos")
# Cargamos los datos
set.seed(2312)
m.file <- file.path(params$folder.data, params$myfile)
load(m.file)
head(HCV4)
#Función
normalizar <- function(x) {
return((x -min(x)) / (max(x) -min(x)))
}
# Particion de los datos train/test
## Muestras que pertenecen al conjunto train
set.seed(77)
n_train <- createDataPartition(HCV4$BH.staging, p = 0.7, list = FALSE)
## Eliminamos las variables con menos importancia
set.seed(77)
HCV2<- HCV4
HCV2 <- HCV2 %>%
select(-Epigastria.pain, -Headache, -Nausea.Vomiting, -Jaundice, -Fatigue.Boneache, -Fever, -Diarrhea, -RNA.12, -RNA.EOT, -RNA.EF, -HGB, -BH.grading, -BMI, -ALT.after.24.w, -ALT.36, -ALT.12, -ALT.48, -ALT.24, -ALT.4, -RNA.4)
## Creamos los conjuntos train y test:
train.d <- HCV2[n_train,]
test.d <- HCV2[-n_train,]
## Variable objetivo por conjunto de datos
label_train <- HCV2[n_train,length(HCV2)]
label_test <- HCV2[-n_train,length(HCV2)]
# Preparamos los conjuntos de datos para que sean preprocesados
# Creamos un respaldo de los datos de prueba y entrenamiento
data_train <- train.d
data_test  <- test.d
data_tn <- data_train
data_tt <- data_test
data_tn <- data_tn %>%
select(-Age, -Gender)
data_tt <- data_tt %>%
select(-Age, -Gender)
# Normalizamos los datos de prueba y entrenamiento
data_tn2 <- data_tn %>%
mutate_if(is.numeric, normalizar)
data_tt2 <- data_tt %>%
mutate_if(is.numeric, normalizar)
# Obtener los valores mínimos y máximos de las variables numéricas en data_train
min_val <- data_tn %>%
summarise(across(where(is.numeric), min))
max_val <- data_tn %>%
summarise(across(where(is.numeric), max))
#save(data_train, data_test, min_vals, max_vals, file = "datos.RData")
library(keras)
library(mlr)
library(tensorflow)
# Crear variables one-hot para la variable de salida
data_tn2$BH.staging.1 <- as.integer(data_tn2$BH.staging == "F1")
data_tn2$BH.staging.2 <- as.integer(data_tn2$BH.staging == "F2")
data_tn2$BH.staging.3 <- as.integer(data_tn2$BH.staging == "F3")
data_tn2$BH.staging.4 <- as.integer(data_tn2$BH.staging == "F4")
data_tn2 <- data_tn2 %>%
select(-BH.staging)
# Convertir a matriz y variables one-hot
set.seed(161121)
nc <- ncol(data_tn2)
x_train <- as.matrix(data_tn2[, 1:nc])
y_train <- as.matrix(data_tn2[, c("BH.staging.1", "BH.staging.2", "BH.staging.3", "BH.staging.4")])
tensorflow::set_random_seed(700)
# Crear modelo secuencial
modelo_sfg <- keras_model_sequential() %>%
layer_dense(units = 32, activation = "relu", input_shape = c(nc)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = "softmax")
# Compilar modelo
#learning_rate <- c(0.001, 0.01, 0.1)
learning_rate <- 0.0001
tensorflow::set_random_seed(700)
modelo_sfg %>% compile(
optimizer = optimizer_adam(learning_rate),
loss = "categorical_crossentropy",
metrics = "accuracy"
)
tensorflow::set_random_seed(700)
# Entrenar el modelo
history <- modelo_sfg %>% fit(
x = x_train,
y = y_train,
epochs = 50,
batch_size = 32,
validation_split = 0.2,
verbose = 0
)
plot(history)
# Crear variables one-hot para la variable de salida
data_tt2$BH.staging.1 <- as.integer(data_tt2$BH.staging == "F1")
data_tt2$BH.staging.2 <- as.integer(data_tt2$BH.staging == "F2")
data_tt2$BH.staging.3 <- as.integer(data_tt2$BH.staging == "F3")
data_tt2$BH.staging.4 <- as.integer(data_tt2$BH.staging == "F4")
data_tt2 <- data_tt2 %>%
select(-BH.staging)
set.seed(161222)
x_test <- as.matrix(data_tt2[, 1:nc])
set.seed(161222)
predicciones_sfg_prob <- predict(modelo_sfg, x_test)
predicciones_sfg <- max.col(predicciones_sfg_prob)
# Convertir predicciones a valores originales de BH.staging
predicciones_sfg <- factor(ifelse(predicciones_sfg == 1, "F1",
ifelse(predicciones_sfg == 2, "F2",
ifelse(predicciones_sfg == 3, "F3", "F4"))),
levels = c("F1", "F2", "F3", "F4"))
set.seed(161222)
conf_mat_sfg <- caret::confusionMatrix(predicciones_sfg, factor(data_test$BH.staging, levels = c("F1", "F2", "F3", "F4")))
conf_mat_sfg
stats_class_sfg <- data.frame(model = "SFG",
precision = conf_mat_sfg$overall["Accuracy"],
FN = conf_mat_sfg$table[2,1],
FP = conf_mat_sfg$table[1,2],
error.rate = 1 - conf_mat_sfg$overall["Accuracy"],
kappa = conf_mat_sfg$overall["Kappa"],
sensibilidad = mean(conf_mat_sfg$byClass[, "Sensitivity"]),
especificidad = mean(conf_mat_sfg$byClass[, "Specificity"]),
precisión = mean(conf_mat_sfg$byClass[, "Pos Pred Value"]),
recuperación = mean(conf_mat_sfg$byClass[, "Recall"]),
f.medida = mean(conf_mat_sfg$byClass[, "F1"])
)
stats_class_sfg
# Tabla resumen del rendimiento de los diferentes modelos:
stats_models <- rbind(stats_class_sfg)
# Ordenamos la tabla por la precisión y lo guardamos
stats_models_prec <- stats_models %>% arrange(desc(precision))
knitr::kable(stats_models_prec, digits = 3, caption = "Métricas del rendimiento de los modelos de aprendizaje automático.")
rsconnect::setAccountInfo(name='andreavacauoc', token='3A706FDC941F4225AC47D0F54A51E709', secret='0v7zUAOYwqKJ6OpngFyXAUZaQhoV8KarYbJHMwki')
install.packages("rsconnect")
library(rsconnect)
rsconnect::setAccountInfo(name='andreavacauoc', token='3A706FDC941F4225AC47D0F54A51E709', secret='0v7zUAOYwqKJ6OpngFyXAUZaQhoV8KarYbJHMwki')
shiny::runApp('D:/UOC/HepatoFactor/src')
runApp('D:/UOC/HepatoFactor/src')
runApp('D:/UOC/HepatoFactor/src')
runApp('D:/UOC/HepatoFactor/src')
knitr::opts_chunk$set(echo = TRUE, comment = NULL,
warning = FALSE, message = FALSE,
fig.align="center")
# Load packages
library(knitr)
library(kableExtra)
library(ggplot2)
library(ggpubr)
library(dplyr)
library(rlang)
# Load script with plots functions
source("graficos_web_report.R")
# Results table
params$data_pred %>%
kable(booktabs = TRUE,format = "latex",digits = 1,
longtable=TRUE) %>%
kable_styling(
latex_options = c("striped", "condensed","repeat_header"),
position = "center",
full_width = FALSE,
repeat_header_text = "Continuation",
repeat_header_continued="Continue on the next page")
# Data table
params$data_raw %>%
kable(booktabs = TRUE,format = "latex",digits = 1,
longtable=TRUE) %>%
kable_styling(
latex_options = c("striped", "condensed","repeat_header"),
position = "center",
full_width = FALSE,
repeat_header_text = "Continuation",
repeat_header_continued="Continue on the next page")
# Pie plots
p1 <- plot_gender(dataset = params$data_pred) # Gender
View(params)
View(params)
# Results table
params$data_pred %>%
kable(booktabs = TRUE,format = "latex",digits = 1,
longtable=TRUE) %>%
kable_styling(
latex_options = c("striped", "condensed","repeat_header"),
position = "center",
full_width = FALSE,
repeat_header_text = "Continuation",
repeat_header_continued="Continue on the next page")
# Pie plots
p1 <- plot_gender(dataset = params$data_pred) # Gender
runApp('D:/UOC/HepatoFactor/src')
