---
title: "Modelo SFG"
author: "Andrea Vaca"
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
output:
  html_document:
    code_folding: show
    toc: yes
    toc_float:
      toc_collapsed: yes
    toc_depth: 4
    theme: flatly
    highlight: textmate
    number_sections: yes
editor:
  markdown:
    wrap: 72
self_contained: yes
header-includes:
- \usepackage{float}
- \floatplacement{figure}{H}
- \usepackage[spanish]{babel}
editor_options: 
  chunk_output_type: inline
params:
  folder.data: "./data"
  myfile1: "datos.RData"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NULL,
                      warning = FALSE, message = FALSE, 
                      fig.align="center", cache = TRUE)
```

```{r paquetes, include=FALSE}
# Cargamos los paquetes
library(knitr)
 library(caret)
 library(class)
 library(e1071)
 library(neuralnet)
 library(NeuralNetTools)
 library(kernlab)
 library(randomForest)
 library(psych)
 library(ggplot2)
 library(gridExtra)
 library(ggpubr)
 library(dplyr)
 library(bookdown)
 library(prettydoc)
 library(corrplot)
 library(ggcorrplot)
 library(GGally)
 library(randomForestSRC)
 library(tibble)
 library(recipes)
 library(tidyr)
 library(outliers)
 library(RSNNS)
 library(ranger)
 library(lattice)
library(keras)
library(mlr)
library(tensorflow)
```

\newpage

```{r Import_data1}
# Cargamos los datos
set.seed(2312)

m.file1 <- file.path(params$folder.data, params$myfile1)
load(m.file1)
head(data_train)

```

### Preprocesamiento de datos

Creamos una función para normalizar los datos en base a mínimos y máximos para que tengan valores entre 0 y 1 y poder observar todos los datos a la misma escala.

```{r, echo=FALSE}
library(recipes)

# Definir el objeto recipe
data_recipe <- recipe(BH.staging ~ ., data = data_train) %>%
  step_rm(Age, Gender, -all_outcomes()) %>%
  step_normalize(all_numeric(), -all_outcomes(), -all_nominal())
  
# Aplicar los pasos de preprocesamiento a los datos de entrenamiento
trained_recipe <- prep(data_recipe, training = data_train)

# Convertir los datos preprocesados en matriz
data_train1 <- bake(trained_recipe, new_data = NULL)

# Crear variables one-hot para la variable de salida
data_train1$BH.staging.1 <- as.integer(data_train1$BH.staging == "F1")
data_train1$BH.staging.2 <- as.integer(data_train1$BH.staging == "F2")
data_train1$BH.staging.3 <- as.integer(data_train1$BH.staging == "F3")
data_train1$BH.staging.4 <- as.integer(data_train1$BH.staging == "F4")

data_train1 <- data_train1 %>% 
  select(-BH.staging)

# Convertir a matriz y variables one-hot
set.seed(161121)
nc <- ncol(data_train1)
x_train <- as.matrix(data_train1[, 1:nc])
y_train <- as.matrix(data_train1[, c("BH.staging.1", "BH.staging.2", "BH.staging.3", "BH.staging.4")])


# Guardar los datos preprocesados
#save(trained_recipe, file = "modelo_datos.RData")
```

```{r}
# Aplicar los pasos de preprocesamiento a los datos de prueba
preprocessed_test <- bake(trained_recipe, new_data = data_test)

# Crear variables one-hot para la variable de salida
preprocessed_test$BH.staging.1 <- as.integer(preprocessed_test$BH.staging == "F1")
preprocessed_test$BH.staging.2 <- as.integer(preprocessed_test$BH.staging == "F2")
preprocessed_test$BH.staging.3 <- as.integer(preprocessed_test$BH.staging == "F3")
preprocessed_test$BH.staging.4 <- as.integer(preprocessed_test$BH.staging == "F4")

preprocessed_test <- preprocessed_test %>% 
  select(-BH.staging)

# Convertir a matriz y variables one-hot
set.seed(161222)
nc <- ncol(preprocessed_test)
x_test <- as.matrix(preprocessed_test[, 1:nc])

```


## Modelo de Sequential Feature Generator (SFG)

```{r}
library(keras)
library(mlr)
library(tensorflow)


tensorflow::set_random_seed(779)
# Crear modelo secuencial
modelo_sfg_recipe <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = "relu", input_shape = c(nc)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 4, activation = "softmax")

# Compilar modelo
#learning_rate <- c(0.001, 0.01, 0.1)

learning_rate <- 0.0001

tensorflow::set_random_seed(779)
modelo_sfg_recipe %>% compile(
  optimizer = optimizer_adam(learning_rate),
  loss = "categorical_crossentropy",
  metrics = "accuracy"
)

tensorflow::set_random_seed(779)
  # Entrenar el modelo
history <- modelo_sfg_recipe %>% fit(
  x = x_train,
  y = y_train,
  epochs = 135,
  batch_size = 32,
  validation_split = 0.2,
  verbose = 0
)

plot(history)

# Guardar el modelo en formato HDF5
#modelo_sfg_recipe_archivo <- "modelo_sfg_recipe.h5"
#keras::save_model_hdf5(filepath = modelo_sfg_recipe_archivo, modelo_sfg_recipe)

```

```{r, cache=TRUE}

set.seed(161222)
predicciones_sfg_prob <- predict(modelo_sfg_recipe, x_test)
predicciones_sfg <- max.col(predicciones_sfg_prob)

# Convertir predicciones a valores originales de BH.staging
predicciones_sfg <- factor(ifelse(predicciones_sfg == 1, "F1",
                                  ifelse(predicciones_sfg == 2, "F2",
                                         ifelse(predicciones_sfg == 3, "F3", "F4"))),
                           levels = c("F1", "F2", "F3", "F4"))
set.seed(161222)
conf_mat_sfg <- caret::confusionMatrix(predicciones_sfg, factor(data_test$BH.staging, levels = c("F1", "F2", "F3", "F4")))
conf_mat_sfg


```

```{r}
stats_class_sfg <- data.frame(model = "SFG",
                              precision = conf_mat_sfg$overall["Accuracy"],
                              FN = conf_mat_sfg$table[2,1],
                              FP = conf_mat_sfg$table[1,2],
                              error.rate = 1 - conf_mat_sfg$overall["Accuracy"],
                              kappa = conf_mat_sfg$overall["Kappa"],
                              sensibilidad = mean(conf_mat_sfg$byClass[, "Sensitivity"]),
                              especificidad = mean(conf_mat_sfg$byClass[, "Specificity"]),
                              precisión = mean(conf_mat_sfg$byClass[, "Pos Pred Value"]),
                              recuperación = mean(conf_mat_sfg$byClass[, "Recall"]),
                              f.medida = mean(conf_mat_sfg$byClass[, "F1"])
)
stats_class_sfg
```

```{r, echo=FALSE}
# Tabla resumen del rendimiento de los diferentes modelos:
stats_models <- rbind(stats_class_sfg)

# Ordenamos la tabla por la precisión y lo guardamos
stats_models_prec <- stats_models %>% arrange(desc(precision))

knitr::kable(stats_models_prec, digits = 3, caption = "Métricas del rendimiento de los modelos de aprendizaje automático.")

```